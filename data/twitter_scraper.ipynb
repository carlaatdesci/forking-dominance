{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFW01Q+6l/5JSDJ8C85iJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlaost/fundingdominance/blob/main/data/twitter_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prep"
      ],
      "metadata": {
        "id": "VQR--VmgQo9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MO3QOUghCho0"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import time\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting user names"
      ],
      "metadata": {
        "id": "Gepxg_YkDw_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_base = pd.read_csv('https://raw.githubusercontent.com/carlaost/fundingdominance/main/data/raw/funding_raw.csv')\n",
        "df_usernames = pd.read_csv('https://raw.githubusercontent.com/carlaost/fundingdominance/main/data/raw/twitter_handles.csv')"
      ],
      "metadata": {
        "id": "oErngDhhEJZP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = df_base[['Deal ID', 'Companies', 'CEO (at time of deal)', 'CEO PBId']].copy()\n",
        "users['username'] = df_usernames['Twitter Handle']"
      ],
      "metadata": {
        "id": "qMOb0LJ3HtDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cleaning user names"
      ],
      "metadata": {
        "id": "mbJzx6EsKDaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(users.shape[0]):\n",
        "  users['username'][i] = re.sub(r'@', '', str(users['username'][i]))\n",
        "  users['username'][i] = re.sub(r'\\*', '', str(users['username'][i]))\n",
        "  if '**' in users['username'][i]:\n",
        "    users['username'][i] = ''\n",
        "  if users['username'][i] == 'nan':\n",
        "    users['username'][i] = ''\n",
        "  if ',' in users['username'][i]: # deleting ambiguous usernames (n=131), unable to verify\n",
        "    users['username'][i] = ''"
      ],
      "metadata": {
        "id": "vbEcJyN0KE07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empty = 0\n",
        "handle = 0\n",
        "for i in range(users.shape[0]):\n",
        "  if users['username'][i] == '':\n",
        "    empty +=1\n",
        "  else:\n",
        "    handle +=1"
      ],
      "metadata": {
        "id": "SYneQAxyaLC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scraper"
      ],
      "metadata": {
        "id": "_GS188dcDobO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usernames = users['username'].tolist()\n",
        "usernames = [x for x in usernames if x]"
      ],
      "metadata": {
        "id": "Leou_p4ebAlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile_info = pd.DataFrame(columns=['usernames', 'pfp_url', 'url_https', 'network_size'])"
      ],
      "metadata": {
        "id": "aUQZBfu6NNol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consumer_key = xxx # add your token\n",
        "consumer_secret = # add your token\n",
        "access_token = # add your token\n",
        "access_token_secret = # add your token"
      ],
      "metadata": {
        "id": "IpZsWn9uDWyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def username_tweets_to_csv(username,count):\n",
        "    try:      \n",
        "        # Creation of query method using parameters\n",
        "        tweets = tweepy.Cursor(api.user_timeline,screen_name=username, tweet_mode=\"extended\", include_rts=False,).items(count)\n",
        "\n",
        "        # Pulling information from tweets iterable object\n",
        "        tweets_list = [[tweet.created_at, tweet.id, tweet.full_text] for tweet in tweets]\n",
        "\n",
        "        # Creation of dataframe from tweets list\n",
        "        # Add or remove columns as you remove tweet information\n",
        "        tweets_df = pd.DataFrame(tweets_list,columns=['Datetime', 'Tweet Id', 'Text'])\n",
        "\n",
        "        # Converting dataframe to CSV \n",
        "        tweets_df.to_csv('/content/drive/MyDrive/funding-dominance/data-collection/tweets/{}-tweets.csv'.format(username), sep=',', index = False)\n",
        "\n",
        "    except BaseException as e:\n",
        "          print(username, '- failed on_status,',str(e))\n",
        "          time.sleep(3)"
      ],
      "metadata": {
        "id": "tCBGIgYCcnHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_info(username):\n",
        "\n",
        "  try:\n",
        "\n",
        "    user_info = api.get_user(screen_name=username)\n",
        "\n",
        "    url = user_info.profile_image_url\n",
        "    url = url.replace('_normal','')\n",
        "    url_https =user_info.profile_image_url_https\n",
        "    network = user_info.followers_count\n",
        "\n",
        "    return url, url_https, network\n",
        "\n",
        "  except BaseException as e:\n",
        "    print(username, '- failed on_status,',str(e))\n",
        "    time.sleep(3)"
      ],
      "metadata": {
        "id": "fxL5SLN3MwPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 25\n",
        "n = len(usernames)\n",
        "batches = n // batch_size\n",
        "batch_rest = n % batches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsaXV1nTsU1s",
        "outputId": "1fbdc827-f2cb-4317-f66c-3bd6948c4725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batches = 81 || batches_size = 25 || batch_rest = 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 3000\n",
        "\n",
        "for batch in range(batches):\n",
        "  start = batch * batch_size\n",
        "  end = start + batch_size - 1\n",
        "\n",
        "  #Twitter API login; re-entering for every batch since this solved errors during testing\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "  #calling tweet scraper for every user\n",
        "  for i in range(start, end):\n",
        "    username = usernames[i]\n",
        "    username_tweets_to_csv(username, count)\n",
        "    url, url_https, network = get_info(username)\n",
        "    row = {'usernames': username, 'pfp_url': url, 'url_https': url_https, 'network_size': network}\n",
        "    data = data.append(row, ignore_index = True)\n",
        "\n",
        "  print('Batch', batch, '/', batches, 'done.')\n",
        "\n",
        "for i in range(n - batch_rest, n):\n",
        "  username = usernames[i]\n",
        "  username_tweets_to_csv\n",
        "  url, url_https, network = get_info(username)\n",
        "  row = {'usernames': username, 'pfp_url': url, 'url_https': url_https, 'network_size': network}\n",
        "  data = data.append(row, ignore_index = True)\n",
        "print('Final batch done.')"
      ],
      "metadata": {
        "id": "e2S4krt9tVza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download pfps"
      ],
      "metadata": {
        "id": "vHBEh0xQN7K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pfp(url, username):\n",
        "  try:\n",
        "    filename = # your directory; make sure to change the filename per user to avoid overriding images\n",
        "    urllib.request.urlretrieve(url,filename)\n",
        "  except:\n",
        "    print('Error for', username)"
      ],
      "metadata": {
        "id": "tjhn8GDLNjqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(data.shape[0]):\n",
        "  url = data['pfp_url'][i]\n",
        "  username = data['usernames'][i]\n",
        "  save_pfp(url, username)"
      ],
      "metadata": {
        "id": "RflUITHcNlvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}