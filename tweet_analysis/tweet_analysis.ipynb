{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet-dominance-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN29FYg5DJKPDgHQfUi4+LG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlaost/fundingdominance/blob/main/tweet_analysis/tweet_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###setup"
      ],
      "metadata": {
        "id": "35Wz8Uxt-UFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text &> /dev/null\n",
        "!pip install transformers &> /dev/null"
      ],
      "metadata": {
        "id": "j7Oug_w6FsLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "-KbI-OTLWDni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "general = pd.read_csv('https://raw.githubusercontent.com/carlaost/fundingdominance/main/data/processed/dealid_faces.csv')\n",
        "df = pd.DataFrame(columns=['deal_id', 'deal_size', 'deal_date', 'ceo_id', 'username', 'network', 'smile', 'tweets_v', 'tweets_a', 'tweets_d', 'tweet_count'])"
      ],
      "metadata": {
        "id": "amjnSdse4HFE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(general)):\n",
        "  if general['tweets_period'][i] == 'ok' or general['tweets_period'][i] == 'hyperactive':\n",
        "    info = {'deal_id': general['Deal ID'][i],\n",
        "            'deal_size': general['Deal Size'][i],\n",
        "            'deal_date': general['Deal Date'][i],  \n",
        "            'ceo_id': general['CEO PBId'][i], \n",
        "            'username': general['usernames_x'][i], \n",
        "            'network': general['network size_x'][i], \n",
        "            'smile': general['smile_type_w_x'][i]}\n",
        "    df = df.append(info, ignore_index = True)"
      ],
      "metadata": {
        "id": "zoHy0c5Y5Jks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###functions"
      ],
      "metadata": {
        "id": "yllgCYGG-ZsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3')\n",
        "  bert = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4', trainable=False)\n",
        "  \n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "  bert_inputs = preprocessor(text_input)\n",
        "  bert_outputs = bert(bert_inputs)\n",
        "  cls_embedding = bert_outputs['pooled_output']\n",
        "\n",
        "  inter1 = tf.keras.layers.Dense(64, name='inter1')(cls_embedding)\n",
        "  inter2 = tf.keras.layers.Dense(64, name='inter2')(inter1)\n",
        "\n",
        "  output1 = tf.keras.layers.Dense(1, name='V')(inter2)\n",
        "  output2 = tf.keras.layers.Dense(1, name='A')(inter2)\n",
        "  output3 = tf.keras.layers.Dense(1, name='D')(inter2)\n",
        "\n",
        "  model = tf.keras.Model(\n",
        "    inputs=text_input, \n",
        "    outputs=[output1, output2, output3])\n",
        "  \n",
        "  optimizer='rmsprop'\n",
        "  loss={'V':'mse', 'A':'mse', 'D':'mse'}\n",
        "  metrics={'V':'mae', 'A':'mae', 'D':'mae'}\n",
        "\n",
        "  model.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "YyqTVmYJFY4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, clean):\n",
        "  output = model.predict([clean])\n",
        "  v = output[0]\n",
        "  v = v[0][0]\n",
        "  a = output[1]\n",
        "  a = a[0][0]\n",
        "  d = output[2]\n",
        "  d = d[0][0]\n",
        "  return v, a, d"
      ],
      "metadata": {
        "id": "dlJPSYesVlkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_info(i):\n",
        "  username = df['username'][i]\n",
        "  deal_date = df['deal_date'][i]\n",
        "  deal_date = datetime.strptime(str(deal_date), '%d-%b-%Y')\n",
        "  return username, deal_date"
      ],
      "metadata": {
        "id": "aZaCGLr7A_Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emotion_analysis(f, deal_date):\n",
        "\n",
        "  tweets = pd.read_csv(f)\n",
        "  result = pd.DataFrame(columns=['text', 'v', 'a', 'd'])\n",
        "  tweet_count = 0\n",
        "\n",
        "  for tweet in range(len(tweets)):\n",
        "    tweet_date = tweets['Datetime'][tweet]\n",
        "    tweet_date = datetime.strptime(str(tweet_date), '%Y-%m-%d %H:%M:%S')\n",
        "    \n",
        "    if tweet_date < deal_date and (deal_date - tweet_date) < (datetime(2022, 5, 27) - datetime(2021, 5, 27)):\n",
        "      clean = re.sub(r'@\\w*', '', str(tweets['Text'][tweet]))\n",
        "      clean = re.sub(r'https\\S*', '', str(clean))\n",
        "      v, a, d = predict(model, clean)\n",
        "      result = result.append({'text': clean, 'v':v, 'a':a, 'd':d}, ignore_index=True)\n",
        "      tweet_count += 1\n",
        "    \n",
        "    else: continue\n",
        "\n",
        "  return result, tweet_count"
      ],
      "metadata": {
        "id": "zsN97w3hXCk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def means(result):\n",
        "  v = result['v'].mean()\n",
        "  a = result['a'].mean()\n",
        "  d = result['d'].mean()\n",
        "  return v, a, d"
      ],
      "metadata": {
        "id": "4td1NbVqu3tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(df, i):\n",
        "\n",
        "  username, deal_date = get_info(i)\n",
        "  \n",
        "  f = 'xxx/{}-tweets.csv'.format(username) # add directory / tweet filepath\n",
        "  if os.path.isfile(f):\n",
        "    result, tweet_count = emotion_analysis(f, deal_date)    \n",
        "    result.to_csv('xxx/{}_analyzed.csv'.format(username)) # add directory / tweet filepath\n",
        "    df['tweets_v'][i], df['tweets_a'][i], df['tweets_d'][i] = means(result)\n",
        "    df['tweet_count'][i] = tweet_count\n",
        "\n",
        "    else:\n",
        "      print(username, ': no file')"
      ],
      "metadata": {
        "id": "rnYCKoDw_dLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###emotion analysis"
      ],
      "metadata": {
        "id": "KGdXJnj3-yRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script iterates through all tweet files, and requires to load the EmoBERT model weights from checkpoint.\n",
        "\n",
        "Tweet files can be obtained using the [tweet scraper](https://github.com/carlaost/fundingdominance/blob/main/data/twitter_scraper.ipynb), EmoBERT checkpoint can be downloaded [here](https://drive.google.com/drive/folders/1-LTXEh-xGuAyNCZbBqtbQ6rjoJKZUd56?usp=sharing). Filepaths need to be adjusted."
      ],
      "metadata": {
        "id": "X62EHMTKXkod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "checkpoint = xxx # filepath to model checkpoint\n",
        "model.load_weights(checkpoint)"
      ],
      "metadata": {
        "id": "dFdHewUdVPYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "batches = len(df) // batch_size\n",
        "rest = len(df) % batches"
      ],
      "metadata": {
        "id": "b5wkEo1JZ4_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in range(batches):\n",
        "  start = batch * batch_size\n",
        "  end = start + batch_size\n",
        "\n",
        "  for i in range(start, end):\n",
        "    run(df, i)\n",
        "\n",
        "  df.to_csv(xxx) # filepath to save output file  \n",
        "  print('Batch no.', batch, '(+1) /', batches, 'done.')\n",
        "\n",
        "for i in range(int(len(df)-rest), len(df)):\n",
        "  run(df, i)\n",
        "\n",
        "df.to_csv(xxx) # filepath to save output file   "
      ],
      "metadata": {
        "id": "yNt9PrBia0lt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}